[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "",
    "text": "Abstract.\nKeywords. svars, impulse responses, quarto, R, monetary policy"
  },
  {
    "objectID": "index.html#objective-and-motivation",
    "href": "index.html#objective-and-motivation",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "Objective and Motivation",
    "text": "Objective and Motivation\nThe goal of this research project is to analyze the impact of unconditional government stipends on the U.S. economy, such as stimulus checks. To identify these effects I will analyze the effects of the stimulus checks and increased unemployment benefits issued by the US government in the wake of the COVID-19 pandemic.\nThis is an important topic as inequality rises direct government action may become increasingly necessary. Increased taxation on the rich and targeted government programs can only do so much to help those at the lower and middle ends of income. Especially as the groups in need of help become larger and their needs more diverse, the high administrative costs of judging who gets the benefits and what they are applicable to may become untenable. In this case, regular stimulus payments or higher unemployment benefits may become necessary to the continued economic health of the country. However, without knowing the effects of these methods, it is difficult to say whether or not they would do more harm than good."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "Data",
    "text": "Data\nFor the purpose of this analysis data the United States Federal Reserve will be utilized. This source uses government data and is considered highly reliable. A potential issue is the information available being limited by the frequency with which the government collects data. To counter this, the estimates will use a long history and quarterly data which is the most frequent commonly available data.\nThe core factors chosen, median income, gdp per capita, and unemployment, were chosen for widely being considered to be strong indicators of economic health. An estimated four lags are used as this is quarterly data so by incorporating four lags seasonal effects are adequately accounted for.\nReal GDP Per Capita, Quarterly: https://fred.stlouisfed.org/series/A939RX0Q048SBEA\nUnemployment Rate, Monthly: https://fred.stlouisfed.org/series/UNRATE\nMedian Usual Weekly Real Earnings: https://fred.stlouisfed.org/series/LES1252881600Q\nConsumer Price Index, Less Food and Energy: https://fred.stlouisfed.org/series/CPILFESL\nReal Government Consumption Expenditures: https://fred.stlouisfed.org/series/A955RX1Q020SBEA\nM2 Real Money Supply: https://fred.stlouisfed.org/series/M2REAL\nMedian income is chosen over average income as, due to income disparity in the united states, there are large distortions in the average compared to the median. As stimulus payments and increased unemployment benefits are likely to more significantly impact lower income individuals it was determined that the median income would prove more suitable.\nGDP per capita is selected as it accounts for fluctuations in population over time instead of including effects that could be simple shifts in total population.\nUnemployment is utilized at it is of paramount concern in either confirming or assuaging concerns that, with relatively less incentive to work, that a portion of the population will elect not to work.\nCPI will reflect the changes in price level as a result of subsidies. Prices of food and energy are ignored as there are a large number of confounding factors which may distort data but are usually short term shocks as opposed to more systemic changes. For example, eggs fluctuated wildly in price in the United States for several months in early 2023 due solely to the conditions of the egg market and not due to any outside effects. For more durable goods it is expected that the changes in price level will be due largely due to more systemic shifts than short-term changes.\nReal government consumption will certainly rise, at least short term, when income rises as a result of the increase in income being due to government subsidy. Of interest is signs of potential decrease in government expenditure in other sectors due to rising incomes, or if no such signs appear.\nM2 money supply is included as contraction or expansion of the money supply can serve as an indicator for several other variables such as investment, interest rates, and spending.\nAll values except unemployment rate are taken as first differences to make them a stationary series. Unemployment rate is already stationary and as such does not require transformation. Both the non-stationary and the stationary series will be shown."
  },
  {
    "objectID": "index.html#preliminary-data-analysis",
    "href": "index.html#preliminary-data-analysis",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "Preliminary Data Analysis",
    "text": "Preliminary Data Analysis\n\n\nADF Tests\n#plot autocorrelation functions\npar(mfrow=c(3,2),mar=c(3,3,3,3))\nacf(urate[,2],main=\"Unemployment rate\")\nacf(mearn[,2],main=\"Weekly Earnings\")\nacf(rgdpcap[,2],main=\"Real GDP per capita\")\nacf(CPI[,2],main=\"CPI\")\nacf(govex[,2],main=\"Government Expenditure\")\nacf(M2[,2],main=\"M2\")\n\n\n\n\n\nADF Tests\n#plot PACF\npacf(urate[,2],main=\"Unemployment rate\")\npacf(mearn[,2],main=\"Weekly Earnings\")\npacf(rgdpcap[,2],main=\"Real GDP per capita\")\npacf(CPI[,2],main=\"CPI\")\npacf(govex[,2],main=\"Government Expenditure\")\npacf(M2[,2],main=\"M2\")\n\n\n\n\n\nADF Tests\npar(mfrow=c(1,1),mar=c(5, 4, 4, 2)) #reset default values\n\n#ADF testing\n#See if there is a way to make ADF tests prettier\n  library(tseries)\n  data=cbind(urate[,2],mearn[1:176,2],rgdpcap[1:176,2],CPI[1:176,2],govex[1:176,2],M2[1:176,2])\n  adf <- as.data.frame(matrix(nrow=6,ncol=3,NA))\n  rownames(adf) <- colnames(data[1:6])\n  colnames(adf) <- c(\"Dickey-Fuller\",\"Lag-order\", \"p-value\")\n  \n  for (i in 1:ncol(data)){\n    adf_tmp                 <-  adf.test(data[,i],k=4)\n    adf[i,\"Dickey-Fuller\"]  <-  round(as.numeric(adf_tmp[1]),3)\n    adf[i,\"Lag-order\"]      <-  as.numeric(adf_tmp[2])\n    adf[i,\"p-value\"]        <-  round(as.numeric(adf_tmp[4]),3)\n  }\n  \n  adf.diff <- as.data.frame(matrix(nrow=6,ncol=3,NA))\n  rownames(adf.diff) <- colnames(data[1:6])\n  colnames(adf.diff) <- c(\"Dickey-Fuller diff\",\"Lag-order diff\", \"p-value diff\")\n  \n  for (i in 1: ncol(data)){\n    tmp.diff                         <-  adf.test(diff(data[,i]),k=3)\n    adf.diff[i,\"Dickey-Fuller diff\"] <-  round(as.numeric(tmp.diff[1]),3)\n    adf.diff[i,\"Lag-order diff\"]     <-  as.numeric(tmp.diff[2])\n    adf.diff[i,\"p-value diff\"]       <-  round(as.numeric(tmp.diff[4]),3)\n  }\n  knitr::kable(cbind(adf, adf.diff), index=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDickey-Fuller\nLag-order\np-value\nDickey-Fuller diff\nLag-order diff\np-value diff\n\n\n\n\nUnemployment Rate %\n-3.059\n4\n0.134\n-6.650\n3\n0.010\n\n\nWeekly Earnings\n-2.867\n4\n0.214\n-6.462\n3\n0.010\n\n\nReal GDP Per Capita\n-2.394\n4\n0.412\n-7.077\n3\n0.010\n\n\nCPI\n-2.611\n4\n0.321\n-2.513\n3\n0.362\n\n\nGovSpend in Billions\n-2.348\n4\n0.431\n-3.459\n3\n0.048\n\n\nM2 Money Supply\n-2.532\n4\n0.354\n-4.271\n3\n0.010\n\n\n\n\n\nAll of the variables of interest show a gradually decaying ACF and no significant PACF. This is consistent with variables that have a strong dependence on prior observed values and no significant dependence on the error of the prior observations. This indicates that an AR model is indeed useful for predicting the behavior of these variables.\nThe ADF tests indicate that all of the processes are unit root non-stationary. This is consistent with expectations as all of the variables visually exhibit a strong upward trend. Unemployment rate being non-stationary is something of a surprise, but it has the smallest p-value at 0.1337. On visual inspection it can be seen that, barring the COVID shock, unemployment rate has been cyclical but downward trending.\nIn light of later changes, presented below are additional graphs showing logged differences.\n\n\n\n\n\nFirst Differences shown below.\n\n\nFirst-Differenced Plots\n#Take first differences\n#It's making me select an end point if I want to use not all of column 1\nn = nrow(mearn)\nmearn = cbind(mearn[2:n,1] ,diff(mearn$`Weekly Earnings`))\n#colname since its renaming and being annoying\ncolnames(mearn) = c(\"Date\", \"Weekly Earnings\")\n\nn = nrow(rgdpcap)\nrgdpcap = cbind(rgdpcap[2:n,1], diff(rgdpcap$`Real GDP Per Capita`))\ncolnames(rgdpcap) = c(\"Date\", \"Real GDP Per Capita\")\n\nn = nrow(CPI)\nCPI = cbind(CPI[2:n,1],diff(CPI$CPI))\ncolnames(CPI) = c(\"Date\", \"CPI\")\n\nn = nrow(govex)\ngovex = cbind(govex[2:n,1],diff(govex$`GovSpend in Billions`))\ncolnames(govex) = c(\"Date\", \"GovSpend in Billions\")\n\nn = nrow(M2)\nM2 = cbind(M2[2:n,1],diff(M2$`M2 Money Supply`))\ncolnames(M2) = c(\"Date\", \"M2 Money Supply\")\n\n#visually demonstrate values\npar(mfrow=c(3,2),mar=c(3,3,3,3))\nplot(urate$Date, urate$`Unemployment Rate %`, type = \"l\", lwd = 2,\n     main = \"US Unemployment Rate Over Time\", xlab = \"Year/Quarter\", \n     ylab = \"Unemployment %\", col = \"blue\")\n\nplot(mearn$`Date` ,mearn$`Weekly Earnings` , type = \"l\", lwd = 2,\n     main = \"US Median Weekly Earnings (2020 Dollars) First Difference\", xlab = \"Year/Quarter\", \n     ylab = \"Weekly Earnings in $\", col = \"blue\")\n\nplot(rgdpcap$Date , rgdpcap$`Real GDP Per Capita` , type = \"l\", lwd = 2,\n     main = \"US Real GDP Per Capita (2020 Dollars) First Difference\", xlab = \"Year/Quarter\", \n     ylab = \"Real GDP in $\", col = \"blue\")\n\nplot(CPI$Date , CPI$CPI, type = \"l\", lwd = 2,\n     main = \"US CPI Over Time (Excluding Food and Energy) First Difference\", xlab = \"Year/Quarter\", \n     ylab = \"CPI (1983 Base)\", col = \"blue\")\n\nplot(govex$Date , govex$`GovSpend in Billions`, type = \"l\", lwd = 2,\n     main = \"US Governement Consumption Over Time First Difference\", xlab = \"Year/Quarter\", \n     ylab = \"Billions of 2020 dollars\", col = \"blue\")\n\nplot(M2$Date , M2$`M2 Money Supply`, type = \"l\", lwd = 2,\n     main = \"US M2 Money Supply Over Time First Difference\", xlab = \"Year/Quarter\", \n     ylab = \"Billions of 2020 dollars\", col = \"blue\")\n\n\n\n\n\nFirst-Differenced Plots\npar(mfrow=c(1,1),mar=c(5, 4, 4, 2)) #reset default values\n\n\nACF, PACF, and ADF test for log difference bellow\n\n\nADF Tests\n#plot autocorrelation functions\npar(mfrow=c(3,2),mar=c(3,3,3,3))\nacf(urate[,2],main=\"Unemployment rate\")\nacf(mearnl[,2],main=\"Weekly Earnings\")\nacf(rgdpcapl[,2],main=\"Real GDP per capita\")\nacf(CPIl[,2],main=\"CPI\")\nacf(govexl[,2],main=\"Government Expenditure\")\nacf(M2l[,2],main=\"M2\")\n\n\n\n\n\nADF Tests\n#plot PACF\npacf(urate[,2],main=\"Unemployment rate\")\npacf(mearnl[,2],main=\"Weekly Earnings\")\npacf(rgdpcapl[,2],main=\"Real GDP per capita\")\npacf(CPIl[,2],main=\"CPI\")\npacf(govexl[,2],main=\"Government Expenditure\")\npacf(M2l[,2],main=\"M2\")\n\n\n\n\n\nADF Tests\npar(mfrow=c(1,1),mar=c(5, 4, 4, 2)) #reset default values\n\n#ADF testing\n#See if there is a way to make ADF tests prettier\n  library(tseries)\n  data=cbind(urate[,2],mearnl[1:176,2],rgdpcapl[1:176,2],CPIl[1:176,2],govexl[1:176,2],M2l[1:176,2])\n  adf <- as.data.frame(matrix(nrow=6,ncol=3,NA))\n  rownames(adf) <- colnames(data[1:6])\n  colnames(adf) <- c(\"Dickey-Fuller\",\"Lag-order\", \"p-value\")\n  \n  for (i in 1:ncol(data)){\n    adf_tmp                 <-  adf.test(data[,i],k=4)\n    adf[i,\"Dickey-Fuller\"]  <-  round(as.numeric(adf_tmp[1]),3)\n    adf[i,\"Lag-order\"]      <-  as.numeric(adf_tmp[2])\n    adf[i,\"p-value\"]        <-  round(as.numeric(adf_tmp[4]),3)\n  }\n  \n  adf.diff <- as.data.frame(matrix(nrow=6,ncol=3,NA))\n  rownames(adf.diff) <- colnames(data[1:6])\n  colnames(adf.diff) <- c(\"Dickey-Fuller diff\",\"Lag-order diff\", \"p-value diff\")\n  \n  for (i in 1: ncol(data)){\n    tmp.diff                         <-  adf.test(diff(data[,i]),k=3)\n    adf.diff[i,\"Dickey-Fuller diff\"] <-  round(as.numeric(tmp.diff[1]),3)\n    adf.diff[i,\"Lag-order diff\"]     <-  as.numeric(tmp.diff[2])\n    adf.diff[i,\"p-value diff\"]       <-  round(as.numeric(tmp.diff[4]),3)\n  }\n  knitr::kable(cbind(adf, adf.diff), index=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDickey-Fuller\nLag-order\np-value\nDickey-Fuller diff\nLag-order diff\np-value diff\n\n\n\n\nUnemployment Rate %\n-3.059\n4\n0.134\n-6.650\n3\n0.01\n\n\nmearnl[1:176, 2]\n-5.841\n4\n0.010\n-11.702\n3\n0.01\n\n\nrgdpcapl[1:176, 2]\n-6.048\n4\n0.010\n-11.113\n3\n0.01\n\n\nCPIl[1:176, 2]\n-2.285\n4\n0.457\n-7.802\n3\n0.01\n\n\ngovexl[1:176, 2]\n-3.858\n4\n0.018\n-10.667\n3\n0.01\n\n\nM2l[1:176, 2]\n-3.775\n4\n0.022\n-8.900\n3\n0.01\n\n\n\n\n\nThe log difference and first difference graphs are identical in shape with only the magnitude being different. This supports both returning similar results in later analysis.\nAs can be seen above, many of the series are reasonably static over time except for the beginning of the COVID-19 pandemic. This sudden, dramatic change can skew data, especially given the relatively small number of observations amplifying the effects of any outliers. As a result a simple SVAR may not return accurate results due to attributing too much weight to COVID shocks since SVAR models have difficulty accounting for exogenous shocks.\nTo compensate for this the variance spike during the early periods of COVID will need to be accounted for."
  },
  {
    "objectID": "index.html#model-and-hypothesis-this-section-is-next-to-be-updated",
    "href": "index.html#model-and-hypothesis-this-section-is-next-to-be-updated",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "Model and Hypothesis THIS SECTION IS NEXT TO BE UPDATED",
    "text": "Model and Hypothesis THIS SECTION IS NEXT TO BE UPDATED\nThe model utilized will be a six variables SVAR with the following specification\n\\[Y = XA + U\\] \\[U|X \\sim i.i.d. MN_{TxN}(0,\\Sigma,\\Omega)\\]\n\\[\\bf{y}_t = \\begin{bmatrix}\n  mearn_t & =Mean\\: Earnings\\\\\n  urate_t & =Unemployment\\: Rate\\\\\n  rgdpcap_t & =Real \\:GDP \\:Per\\: Capita\\\\\n  CPI_t & =Consumer\\: Price\\: Index\\\\\n  govex_t & =Government\\: Consumption \\:Expenditure\\\\\n  M2_t & =M2\\: Money\\: Supply\n\\end{bmatrix}\\]\nIn this model four lags are utilized, bringing the final relationship to\n\\[\\bf{y}_t = \\bf{y}_{t-1}\\boldsymbol\\alpha_1 + \\bf{y}_{t-2}\\boldsymbol\\alpha_2 + \\bf{y}_{t-3}\\boldsymbol\\alpha_3 + \\bf{y}_{t-4}\\boldsymbol\\alpha_4 + U_t\\]\nt subscripts indicate the time period relative to the present. E.g. t-1 indicates the value of the variable one period in the past.\nThis model goes to four lags as the data is quarterly and this ensures that seasonality effects are removed. The model will serve to identify the effects of shocks on the US economy stemming from stimulus payments as stimulus payments may be treated as one period shocks to income and the impulse response function will tell us the effects that such payments have on the economy.\nEstimating the effects of increased unemployment benefits is more difficult but can be achieved by treating these as a wage floor as presumably if one would be paid less than unemployment benefits one will choose not to work except as necessary to maintain benefits. Thus median income can be replaced with minimum wage in the above calculation and high levels of unemployment benefits treated as an effective increase in the minimum wage. As an alternative, the effects of a universal basic income program may be estimated as a permanent increase in income to all members of society regardless of their employment status.\nThese are all relevant to the economic situation in the United States going forward. A combination of stagnating wages, low minimum wage, and increasing income inequality threatens to force more forceful government action to avert economic crisis stemming from a lower-class which no longer lives at a subsistence level. While this state has not yet been reached it is a looming threat which must be addressed. The number of government programs to help low income individuals is immense but oftentimes much time and energy is spent ensuring that the “undeserving” are not given these benefits which can lead to those in need being rejected or ending up in worse circumstances due to long delays in receiving assistance. All of the proposed methods due to their weak targeting requirements would provide relief more rapidly and potentially aid in economic growth more than programs targeted at covering expenses related to a specific aspect of life.\nSign restrictions will be utilized to conform to the usual thought process of the economic situation in the United States. * indicates an unrestricted relationship\n\\[\\begin{bmatrix}\n  Earnings & Urate & RGDP & CPI & Govex & M2 \\\\\n  * & * & * & * & * & *\\\\\n  1 & * & * & * & * & *\\\\\n  1 & * & * & * & * & *\\\\\n  1 & * & * & * & * & *\\\\\n  1 & * & * & * & * & *\\\\\n  * & * & * & * & * & *\n\\end{bmatrix}\\]\nAs the shock is to earnings, all other relations are free. By many in the US it is believed that higher wages would lead to higher unemployment. There is evidence that GDP rises when wages are increased. It stands to reason the higher pay would lead to inflation. Government consumption is positive as the income change comes from government spending. M2 money supply has several conflicting effects and it is unclear which would dominate so it is left unrestricted.\n\n\nShow code\n#I hope this code works\n\n#equalize length of data\n#keep only values past the date of the newest data series\n#keep only values before 2023\n\n#Comment out to see if I can cut this\n\n#n = nrow(urate)\n#urate = urate[2:n,]\n\n#Honestly, if I were smart this would be some fancy function, I'm not, so it isn't\n\nstart.date = '1979 Q2'\nend.date = '2023 Q1'\n\nmearn = mearn %>% filter(mearn$Date >= start.date)\nmearn = mearn %>% filter(mearn$Date < end.date)\n\nurate = urate %>% filter(urate$Date >= start.date)\nurate = urate %>% filter(urate$Date < end.date)\n\nrgdpcap = rgdpcap %>% filter(rgdpcap$Date >= start.date)\nrgdpcap = rgdpcap %>% filter(rgdpcap$Date < end.date)\n\n\n#CPI doesn't work, but if I change name and change back it works\nCPI2 = CPI\n\nCPI2 = CPI2 %>% filter(CPI2$Date >= start.date)\nCPI = CPI2 %>% filter(CPI2$Date < end.date)\n\ngovex = govex %>% filter(govex$Date >= start.date)\ngovex = govex %>% filter(govex$Date < end.date)\n\nM2 = M2 %>% filter(M2$Date >= start.date)\nM2 = M2 %>% filter(M2$Date < end.date)\n\n#set up y matrix\n#doing it like this so I can toggle stuff off and on\ny = cbind(mearn[,2], urate[,2])\ny = cbind(y, rgdpcap[,2])\ny = cbind(y, CPI[,2])\ny = cbind(y, govex[,2])\ny = cbind(y, M2[,2])\n\n\n#Need to set y as matrix or X becomes wrong object type for future calcs\ny = as.matrix(y)\n#use n for anywhere I need the length of the data\n#this allows for multiple forms of data to be used with varying end dates\nn = nrow(y)\n#To ensure consistency later in setting Sign Restrictions\n#y has variables in the following order\n#Mean Earnings\n#Unemployment Rate\n#Real GDP Per Capita\n#Consumer Price Index\n#Real Government Consumption Expenditure\n#M2 Money Supply\n#Change the above as needed to ensure all sign restrictions are on the left\n\n\n#create VAR model?\n#VAR(4) for 1 year of lags\n\n#It might be possible to cut this depending on if I actually use it anywhere\n\n# setup\n############################################################\n\n#number of variables\nN       = 6\n#number of lags\np       = 4\n#number of draws\n#I am insane this is like 6 hour run time at 50,000\nS       = 20000\n#not the foggiest\nh       = 8\n\n\n# create Y and X\n############################################################  \n\nY       = ts(y[(p+1):n,])\nX       = matrix(1,nrow(Y),1)\nfor (i in 1:p){\n  X     = cbind(X,y[5:n-i,])\n}\n\nt0          = proc.time() # read processor time\n\n\n\n#This might be junk code but I worry about removing it and breaking something\n#Leaving for now, will remove later if I have time\n\n# MLE\n############################################################\nA.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\nSigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n\n#kappa 1 is 1 if non-stationary\n#smaller than 1 if stationary\nkappa.1     = 1^2\n#k2 = 100 is given for Minnesota Prior\nkappa.2     = 100\n#kappa 3 is 1 just given the fact that it determines A.prior\n#which invovles the identity matrix\nkappa.3     = 1\nA.prior     = matrix(0,nrow(A.hat),ncol(A.hat))\n#bit in brackets is just 2:N+1\nA.prior[2:7,] = kappa.3*diag(N)\nV.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\nS.prior     = diag(diag(Sigma.hat))\nnu.prior    = N+1\n\n\n# normal-inverse Wishard posterior parameters\n############################################################\n\n#This is all just given and can be reused infinitely\nV.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))\nV.bar       = solve(V.bar.inv)\nA.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)\nnu.bar      = nrow(Y) + nu.prior\nS.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\nS.bar.inv   = solve(S.bar)\n\n#Ok so all of that above should get me all the posterior distribution\n#Now I need to actually do anything whatsoever with sign restrctions\n\n# Estimating models with sign restrictions: example\n# Use Algorithm 2\n############################################################\nset.seed(123456)\n#column by column\nsign.restrictions = c(0,1,1,1,1,0)\n#this is fine just regardless of how many there are\nR1            = diag(sign.restrictions)\n#Check how to derive\n#I think this is just the posterior\nA = A.bar\n#check how to derive\n#pretty sure this is also just a posterior\nSigma         = S.bar"
  },
  {
    "objectID": "index.html#analysis",
    "href": "index.html#analysis",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\n\nAs can be seen, when weekly earnings is shocked, it tends to cause a shock in the opposite direction in the next period, followed by a small increase, and then stabilizing such that by the end of the second year an income shock creates no further changes in the economy. Curiously, unemployment rate also falls and RGDP experiences a spike before returning to no continuing effect. M2 Money Supply is decreased and suffers from a decreased future growth as well.\nCPI and Government Expenditure show no significant effects in this model.\nOne possible explanation for these changes seeming contradictory to initial assumptions is the holiday worker rush and holiday bonuses interacting with the data. This seasonality should be controlled for by the lag order but I can think of no other viable explanation. In the United States, there is a significant upsurge in employment during the time right around Christmas in December, both in the lead up and during Christmas. During this time is also when many bonuses are paid to workers. And an upsurge in employment is consistent with a spike in RGDP per capita.\nAnother possible reason is the volatility spike from COVID completely subsuming all other effects so that the real effects cannot be observed.\nFurther analysis is needed as I am unclear why these results are occurring in what should be the absence of seasonality and sign restrictions. To test this I will create a second set of Impulse Response Functions including only pre-COVID effects. This will also help me tell what form I expect my IRFs to take after adjusting for COVID.\nIRFS using only pre 2020 data\n\n\n\n\n\nBy removing the post-COVID data it can be seen that the shock to earnings results in a very different pattern. This is consistent with the belief that the large COVID shock has a distortionary effect on all estimations. In this case a shock to weekly earnings results in only a very small effect moving forward, as well as a small decrease in the unemployment rate which adjusts over time but never returns to no change. RGDP per capita shows a similar response regardless of if COVID data is included or not, the main difference is that it does not exhibit a large counter-shock at any point. CPI appear uninfluenced by Weekly Earnings, as does Government Spending. M2 money supply exhibits a downward shift, but to a much lesser extent than the previous model.\nMoving forward into the next model, COVID-19 data will be included but it will be treated as a shock to variance. This ideally allows for the most accurate model yet as it uses the full data set but does not suffer from large distortionary effects.\n\n\nShow code\n#THROWING IN MORE STOLEN CODE\n#analyze this block by block tomorrow\n\n#ensure that proper variables are all utilized as our data is different\n#And I don't trust code reusability (and so he was correct)\n#make sure it all runs and returns expected results\n\n#Ah he's auto culling data\n#I can axe some of this\n#I define Y as an external variable, \n\n#I need to verify I know exactly what this is doing\n#Ideally I can just shift a few variable names around and call it good\n\n#------------------------------\n#V.Posterior\n#------------------------------\n#p = lags\n#k1 and k2 are Minnesota prior\n#start date needs to be tied back to my default\n#h = forecast horizon\nv.posterior.mode <- function(Y, X, p=4, k1=1, k2=100){\n\n  v.neglogPost <- function(theta){\n    N = ncol(Y)\n    # p = no. of lags\n    K = 1 + p*N\n    # forecast horizon\n    # h       = 8\n    T = nrow(Y)\n    \n    # Calculate MLE for prior \n    ############################################################\n    A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\n    Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n    # round(A.hat,3)\n    # round(Sigma.hat,3)\n    # round(cov2cor(Sigma.hat),3)\n    \n    # Specify prior distribution\n    ############################################################\n    kappa.1     = k1\n    kappa.2     = k2\n    kappa.3     = 1\n    A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))\n    A.prior[2:(N+1),] = kappa.3*diag(N)\n    V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\n    S.prior     = diag(diag(Sigma.hat))\n    nu.prior    = N+1\n    \n    vec <- theta[1:3]\n    for (i in 4:12){\n      vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))\n    }  \n    \n    V <- c(ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) , vec)    \n    \n    Y.tilde <- diag(1/V)%*%Y\n    X.tilde <- diag(1/V)%*%X\n    A.tilde.hat <- solve((t(X.tilde)%*%X.tilde+solve(V.prior)))%*%(t(X.tilde)%*%Y.tilde+solve(V.prior)%*%A.prior)\n    epsilon.tilde <-Y.tilde - X.tilde%*%A.tilde.hat\n    \n    # Log-likelihood      \n    logL <- log(prod(V^(-N)))+(-N/2)*log(det(t(X.tilde)%*%X.tilde+solve(V.prior)))+\n            (-(T-p+nu.prior)/2)*log(det(S.prior +t(epsilon.tilde)%*%epsilon.tilde + \n            t(A.tilde.hat-A.prior)%*%solve(V.prior)%*%(A.tilde.hat-A.prior)))\n    \n    # Pareto(1,1) and Beta(3,1.5) priors \n    pareto.a=1\n    pareto.b=1\n    beta.a=3\n    beta.b=1.5\n    beta.cons <- 1/beta(beta.a,beta.b)\n    \n    # Log-prior\n    logP <- log((pareto.a*pareto.b^pareto.a)/(theta[1]^(pareto.a+1))*\n    (pareto.a*pareto.b^pareto.a)/(theta[2]^(pareto.a+1))*\n    (pareto.a*pareto.b^pareto.a)/(theta[3]^(pareto.a+1))*\n    beta.cons*theta[4]^(beta.a-1)*(1-theta[4])^(beta.b-1))\n    \n    # negative log-posterior\n    neglogPost <- -(logL+logP)\n    \n    return(neglogPost)\n  }\n   \n  # numerically minimize the negative log-likelihood\n  post.maximizer <- optim(par=c(50, 50, 50, 0.5), fn=v.neglogPost, method=\"L-BFGS-B\", \n                          lower=c(1, 1, 1, 0.0001),\n                          upper=c(100,100,100,0.99999), hessian = TRUE)\n  \n  return(list(maximizer=post.maximizer$par, hessian=post.maximizer$hessian))\n\n}\n\n\n#------------------------------\n#METRO-HASTINGS\n#------------------------------\nmh.mcmc <- function(Y, X, p=1, S.mh = 1000, c, W = diag(4), theta.init,\n                    k1 = 1, k2 = 100){\n # N = no. of variables\n  N = ncol(Y)\n  # p = no. of lags\n  K = 1 + p*N\n  # forecast horizon\n  # h       = 8\n  T = nrow(Y)\n  \n\n  # Calculate MLE for prior \n  ############################################################\n  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\n  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n  # round(A.hat,3)\n  # round(Sigma.hat,3)\n  # round(cov2cor(Sigma.hat),3)\n  \n  # Specify prior distribution\n  ############################################################\n  kappa.1     = k1\n  kappa.2     = k2\n  kappa.3     = 1\n  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))\n  A.prior[2:(N+1),] = kappa.3*diag(N)\n  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\n  S.prior     = diag(diag(Sigma.hat))\n  nu.prior    = N+1\n  \n  # Metropolis-Hastings \n  ###########################################################\n  # v0, v1, v2, rho\n  Theta <- matrix(NA,S.mh,4)\n  theta_old <- theta.init\n  #theta_old <- Theta[nrow(Theta),]\n  \n  # W <- diag(4)\n  set.seed(69420)\n  pb = txtProgressBar(min = 0, max = S.mh, initial = 0) \n  for (s in 1:S.mh){\n    setTxtProgressBar(pb,s)\n\n    covid.vec <- function(theta){\n      vec <- theta[1:3]\n      for (i in 4:12){\n        vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))\n      }\n      \n      return(vec)\n    }\n\n    # Covid volatility likelihood kernel\n    v.logL <- function(V){\n      Y.tilde <- diag(1/V)%*%Y\n      X.tilde <- diag(1/V)%*%X\n      A.tilde.hat <- solve((t(X.tilde)%*%X.tilde+solve(V.prior)))%*%(t(X.tilde)%*%Y.tilde+solve(V.prior)%*%A.prior)\n      epsilon.tilde <-Y.tilde - X.tilde%*%A.tilde.hat\n\n      logL <- log(prod(V^(-N)))+(-N/2)*log(det(t(X.tilde)%*%X.tilde+solve(V.prior)))+\n              (-(T-p+nu.prior)/2)*log(det(S.prior +t(epsilon.tilde)%*%epsilon.tilde + \n              t(A.tilde.hat-A.prior)%*%solve(V.prior)%*%(A.tilde.hat-A.prior)))\n\n      return(logL)\n    }\n  \n    # Covid volatility prior\n    v.logP <- function(theta, pareto.a=1, pareto.b=1, beta.a=3, beta.b=1.5){\n      beta.cons <- 1/beta(beta.a,beta.b)\n  \n      logP <- log((pareto.a*pareto.b^pareto.a)/(theta[1]^(pareto.a+1))*\n      (pareto.a*pareto.b^pareto.a)/(theta[2]^(pareto.a+1))*\n      (pareto.a*pareto.b^pareto.a)/(theta[3]^(pareto.a+1))*\n       beta.cons*theta[4]^(beta.a-1)*(1-theta[4])^(beta.b-1))\n      \n      return(logP)\n    }\n\n    v_ones <- ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) \n    V.old <- c(v_ones, covid.vec(theta_old))    \n      \n    # New candidate parameters values\n    theta_new <- mvrnorm(1, theta_old, c*W)\n    V.new <- c(v_ones, covid.vec(theta_new))\n    \n    # Calculate posteriors \n    v.logpost_old <- v.logL(V.old)+v.logP(theta_old)\n    v.logpost_new <- v.logL(V.new)+v.logP(theta_new)\n    \n    # Posterior ratio\n    post.ratio <- exp(v.logpost_new-v.logpost_old)\n    \n    # Acceptance/rejection alpha\n    alpha <- min(1, post.ratio)\n    \n    u_star <- runif(1)\n    \n    if (alpha > u_star){\n      Theta[s,] <- theta_new\n    } else {Theta[s,] <- theta_old}\n    \n    theta_old <- Theta[s,]  \n  }\n  \n  colnames(Theta) <- c(\"h0\", \"h1\" , \"h2\", \"rho\")\n\n  re <- list(Theta=Theta, \n             AcceptRate = 1 - rejectionRate(as.mcmc(Theta[,1])))\n  return(re)\n}\n\n#------------------------------\n#Sign Extension Code\n#------------------------------\nsign.extension <- function(Y, X, p=4, S=100,  sign.restrictions = R1,\n                           k1=1, k2=100, shockvar = 1, Theta.mh){\n\n  # N = no. of variables\n  N = ncol(y)\n  # p = no. of lags\n  K = 1 + p*N\n  # forecast horizon\n  # h       = 8\n  \n  \n  covid.vec <- function(theta){\n    vec <- theta[1:3]\n    for (i in 4:12){\n      vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))\n    }\n      \n    return(vec)\n  }\n  \n\n  # array of S diag(covid volatility) matrices\n  \n  #add an extra row, delete the first to fix this\n  diagV.sqinv <- array(NA, c(nrow(Y),nrow(Y),S))\n  \n  for (s in 1:S){\n    v_ones <- ts(rep(1, nrow(Y)-12), 1) \n    diagV.sqinv[,,s] <- diag(c(v_ones, covid.vec(Theta.mh[s,]))^(-2))\n  }\n  \n  # Calculate MLE for prior \n  ############################################################\n  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y\n  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)\n  # round(A.hat,3)\n  # round(Sigma.hat,3)\n  # round(cov2cor(Sigma.hat),3)\n  \n  # Specify prior distribution\n  ############################################################\n  kappa.1     = k1\n  kappa.2     = k2\n  kappa.3     = 1\n  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))\n  A.prior[2:(N+1),] = kappa.3*diag(N)\n  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))\n  S.prior     = diag(diag(Sigma.hat))\n  nu.prior    = N+1\n  \n  # Posterior draws \n  ############################################################\n  Sigma.posterior   = array(NA,c(N,N,S))\n  A.posterior       = array (NA,c(K,N,S))\n  B0.posterior       = array(NA,c(N,N,S))\n  Bplus.posterior       = array(NA,c(N,K,S))\n  \n  pb = txtProgressBar(min = 0, max = S, initial = 0) \n  for (s in 1:S){\n    setTxtProgressBar(pb,s)\n    V.bar.inv   = t(X)%*%diagV.sqinv[,,s]%*%X + diag(1/diag(V.prior))\n    V.bar       = solve(V.bar.inv)\n    A.bar       = V.bar%*%(t(X)%*%diagV.sqinv[,,s]%*%Y + diag(1/diag(V.prior))%*%A.prior)\n    nu.bar      = nrow(Y) + nu.prior\n    S.bar       = S.prior + t(Y)%*%diagV.sqinv[,,s]%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%\n                  A.prior - t(A.bar)%*%V.bar.inv%*%A.bar\n    S.bar.inv   = solve(S.bar)\n    L                 = t(chol(V.bar))\n    \n    # RF posterior draws\n    Sigma.posterior[,,s] <- solve(rWishart(1, df=nu.bar, Sigma=S.bar.inv)[,,1])\n    cholSigma.s     = chol(Sigma.posterior[,,s])\n    A.posterior[,,s]       = matrix(mvrnorm(1,as.vector(A.bar), Sigma.posterior[,,s]%x%V.bar),ncol=N)\n    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s\n    \n    # SF posterior draws \n    B0.posterior[,,s]= solve(t(cholSigma.s))\n    # Draw Bplus\n    Bplus.posterior[,,s] = B0.posterior[,,s]%*%t(A.posterior[,,s])\n  }\n  \n  \n  # Identification via sign restrictions on theta0\n  ############################################################\n\n  # generate corresponding R matrix\n  R1 = diag(sign.restrictions)\n  \n  # storage matrices for Q identified estimates\n  i.vec <- c()\n  Q.iden   = array(NA,c(N,N,S))\n  B0.iden = array(NA,c(N,N,S))\n  B1.iden = array(NA,c(N,K,S))\n  A.iden = array (NA,c(K,N,S))\n  \n  pb = txtProgressBar(min = 0, max = S, initial = 0) \n\n  for (s in 1:S){\n    \n    setTxtProgressBar(pb,s)\n    \n    # pick-up a B0 from S\n    B0.tilde <- B0.posterior[,,s]\n    IR.0.tilde    = solve(B0.tilde)\n    B1.tilde      = Bplus.posterior[,,s]\n    #IR.1.tilde    = solve(B0.tilde)%*%B1.tilde%*%solve(B0.tilde)\n\n    # Search for appropriate Q \n    sign.restrictions.do.not.hold = TRUE\n    i=1\n    while (sign.restrictions.do.not.hold){\n      X           = matrix(rnorm(N^2),N,N)\n      QR          = qr(X, tol = 1e-10)\n      Q           = qr.Q(QR,complete=TRUE)\n      R           = qr.R(QR,complete=TRUE)\n      Q           = t(Q %*% diag(sign(diag(R))))\n      B0          = Q%*%B0.tilde\n      B1          = Q%*%B1.tilde\n      B0.inv      = solve(B0)\n      check       = prod(R1 %*% B0.inv %*% diag(N)[,shockvar] >= 0)\n      A           = t(solve(B0)%*%B1)\n\n      if (check==1){sign.restrictions.do.not.hold=FALSE}\n      i=i+1\n    }\n    i.vec <- c(i.vec, i)\n    Q.iden[,,s] <- Q\n    B0.iden[,,s] <- B0\n    B0.mean <- apply(B0.iden,1:2,mean)\n    B1.iden[,,s] <- B1\n    B1.mean <- apply(B1.iden,1:2,mean)\n    A.iden[,,s] <- A\n    A.mean <- apply(A.iden,1:2,mean)\n  }\n\n  re <- list(\"i\" = i.vec, \"Q\" = Q.iden, \"B0\"= B0.iden, \"B0.mean\" = B0.mean,\n             \"Bplus\"= B1.iden, \"Bplus.mean\" = B1.mean, \"A\" = A.iden, \"A.mean\" = A,\n             \"A.posterior\"=A.posterior, \"Sigma.posterior\"=Sigma.posterior, \"Theta\"= Theta.mh)\n  return(re)\n  \n}\n\n\n#------------------------------\n#IRF plot\n#------------------------------\n\nirf.plot <- function(A.posterior, B0.posterior, shock.var, p=4, h = 12,\n                     varnames = varname_vec){\n  # Define colors\n  ############################################################\n  mcxs1  = \"#05386B\"\n  mcxs2  = \"#379683\"\n  mcxs3  = \"#5CDB95\"\n  mcxs4  = \"#8EE4AF\"\n  mcxs5  = \"#EDF5E1\"\n  purple = \"#b02442\"\n  \n  mcxs1.rgb   = col2rgb(mcxs1)\n  mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)\n  mcxs2.rgb   = col2rgb(mcxs2)\n  mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)\n  \n  # Impulse response functions\n  ############################################################\n  N <- dim(A.posterior)[2]\n  S <- dim(A.posterior)[3]\n  \n  # transform B0 matrices to B\n  B.posterior <- array(NA, c(N,N,S))\n  for (s in 1:S){\n  B.posterior[,,s] <- solve(B0.posterior[,,s])\n  }\n  \n  IRF.posterior     = array(NA,c(N,N,h+1,S))\n  IRF.inf.posterior = array(NA,c(N,N,S))\n  J                 = cbind(diag(N),matrix(0,N,N*(p-1)))\n  \n  pb = txtProgressBar(min = 0, max = S, initial = 0) \n  for (s in 1:S){\n    setTxtProgressBar(pb,s)\n    # define A matrix in VAR(1) representation\n    A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))\n    IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]\n    A.bold.power    = A.bold\n    for (i in 1:(h+1)){\n      if (i==1){\n        IRF.posterior[,,i,s]        = B.posterior[,,s]\n      } else {\n        IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]\n        A.bold.power                = A.bold.power %*% A.bold\n      }\n    }\n  }\n  \n  # save IRFs\n  save(IRF.posterior,IRF.inf.posterior, file=\"irf-k1.RData\")\n\n  # IRF plots GFCF\n  ############################################################\n  IRFs.k1           = apply(IRF.posterior[,shock.var,,],1:2,mean)\n  IRFs.inf.k1       = apply(IRF.inf.posterior[,shock.var,],1,mean)\n  rownames(IRFs.k1) = varnames\n  \n  IRFs.k1.hdi    = apply(IRF.posterior[,shock.var,,],1:2,hdi, credMass=0.68)\n  hh          = 1:(h+1)\n\npar(mfrow=c(3,2), mar=c(3,3.5,2,2),cex.axis=1.5, cex.lab=1.5)\nfor (n in 1:N){\n  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])\n  plot(hh,IRFs.k1[n,hh], type=\"l\", ylim=ylims, axes=FALSE, xlab=\"\", ylab=\"\",\n       main=rownames(IRFs.k1)[n])\n  abline(h = 0, col = \"firebrick\")\n  if (n==N-1 | n==N){\n    axis(1,c(1,2,4,8,12,13),c(\"1 quarter\",\"\",\"1 year\",\"2 years\",\"3 years\",\"\"),cex.axis=0.9)\n  } else {\n    axis(1,c(1,2,4,8,12,13),c(\"1 quarter\",\"\",\"1 year\",\"2 years\",\"3 years\",\"\"),cex.axis=0.9)\n  }\n  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))\n  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=adjustcolor(\"#05386B\", alpha.f = 0.5),border=\"salmon\")\n  lines(hh, IRFs.k1[n,hh],lwd=2,col=\"red\")\n}\n}\n#------------------------------\n#Reset data\n#------------------------------\n\n#Filter data\n\nstart.date = '1979 Q2'\nend.date = '2023 Q1'\n\nmearnl = mearnl %>% filter(mearnl$Date >= start.date)\nmearnl = mearnl %>% filter(mearnl$Date < end.date)\n\nurate = urate %>% filter(urate$Date >= start.date)\nurate = urate %>% filter(urate$Date < end.date)\n\nrgdpcapl = rgdpcapl %>% filter(rgdpcapl$Date >= start.date)\nrgdpcapl = rgdpcapl %>% filter(rgdpcapl$Date < end.date)\n\n#CPI doesn't work, but if I change name and change back it works\n#Yes this is an ongoing issue, don't ask me\nCPI2 = CPIl\n\nCPI2 = CPI2 %>% filter(CPI2$Date >= start.date)\nCPIl = CPI2 %>% filter(CPI2$Date < end.date)\n\ngovexl = govexl %>% filter(govexl$Date >= start.date)\ngovexl = govexl %>% filter(govexl$Date < end.date)\n\nM2l = M2l %>% filter(M2l$Date >= start.date)\nM2l = M2l %>% filter(M2l$Date < end.date)\n\n#set up y matrix\n#doing it like this so I can toggle stuff off and on\ny = cbind(mearnl[,2], urate[,2])\ny = cbind(y, rgdpcapl[,2])\ny = cbind(y, CPIl[,2])\ny = cbind(y, govexl[,2])\ny = cbind(y, M2l[,2])\n\n#Need to set y as matrix or X becomes wrong object type for future calcs\ny = as.matrix(y)\n#use n for anywhere I need the length of the data\n#this allows for multiple forms of data to be used with varying end dates\nn = nrow(y)\n#To ensure consistency later in setting Sign Restrictions\n#y has variables in the following order\n#Mean Earnings\n#Unemployment Rate\n#Real GDP Per Capita\n#Consumer Price Index\n#Real Government Consumption Expenditure\n#M2 Money Supply\n#Change the above as needed to ensure all sign restrictions are on the left\n\n# create Y and X\n############################################################  \n\nY       = ts(y[(p+1):n,])\nX       = matrix(1,nrow(Y),1)\nfor (i in 1:p){\n  X     = cbind(X,y[5:n-i,])\n}\n\n#------------------------------\n#Final extension and plot\n#------------------------------\n\n#p = 4 is default, no need to re-enter, only raises odds of mis-entered data\nextension.post.mode <- v.posterior.mode(Y, X, k1 = 1, k2 = 100)\n\nset.seed(69420)\nextension.mcmc <-mh.mcmc(Y, X, p=4, c=0.000228, W = solve(extension.post.mode$hessian), \n                theta.init = extension.post.mode$maximizer, k1 = 1, S.mh = 20000)\n\n\n================================================================================\n\n\nShow code\nplot.ts(extension.mcmc$Theta, main = \"Metropolis MCMC draws\")\n\n\n\n\n\nShow code\nextended.model <- sign.extension(Y, X, S = 10000, sign.restrictions = R1, k1 = 1, \n                                 Theta.mh = extension.mcmc$Theta[10001:20000,])\n\n\n================================================================================================================================================================\n\n\nShow code\nirf.plot(A.posterior = extended.model$A.posterior, B0.posterior = extended.model$B0, shock.var = 1)\n\n\n================================================================================\n\n\n\n\n\nThe above graphs are generated using log differences in contrast to previous data.Urate continues to be in percentage terms and is unchanged. This is due to using simply first difference data returning extremely aberrant results resulting in rapid, exponential changes when impulse response functions were estimated. No other solution to this problem was found. The same sign restrictions as before are utilized.\nThe current impulse response functions show that there is a minimal ongoing effect on weekly earnings. That is that a one period shock tends to persist in full but lacks any significant ongoing effects on earnings. Unemployment experiences a shift to a higher level that slowly rises over time before begging to decrease once more. The time horizon on this shift is relatively long, implying that the declining wages in real terms due to inflation could be the cause of unemployment rate lowering over time. Real GDP Per Capita shows a slight change , initially a drop before it rises slightly once more, but the impact is very small. Interestingly, CPI does show a small decrease, a possible explanation for this is that changes in output may result in higher productivity, offsetting inflation due to higher wages. Government spending exhibits no strong changes, not does M2 money supply, though M2 money supply does stay slightly above 0 so the long-term impact may be large.\nFor more meaningful comparison, the basic model using log differences is shown below\n\n\n\n\n\nAn important note, as these are log differences is that a one period shock still has ongoing impacts, unlike if it were in absolute terms. In absolute terms, a one period shock followed by an IRF that stays close to zero would mean that the effect is only present for one period. In first differences, a one period shock followed by an IRF that stays close to zero would mean that the shock causes an overall shift in the level on a permanent basis, but doesn’t adjust the future differences. This is why a one period shock to income in this case shows an ongoing impact on unemployment, but no effect on income.\nThis does put limitations on the model as it cannot adequately account of the effects of a one period increase in income, such as stimulus checks, that have no ongoing effects whatsoever. However it can potentially account for other ongoing changes as a universal basic income or increased unemployment benefits. Curiously, government spending barely changes at all. This may be an indicator that, while the money is coming from the government, other benefits may be cut to cause the total change in spending to come out to net zero or very close to it."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "The Effects of Increased Unemployment Benefits and Stimulus Checks on the US Economy",
    "section": "References",
    "text": "References\nLenza, M., Primiceri, G. E. (2022). How to estimate a vector autoregression after March 2020. Journal of Applied Econometrics, 37( 4), 688– 699.https://doi.org/10.1002/jae.2895"
  }
]